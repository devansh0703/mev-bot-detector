// services/backend/index.js (Corrected Version)
const { ethers } = require('ethers');
const { Kafka } = require('kafkajs');
const { createClient } = require('redis');
const { request, gql } = require('graphql-request');

const config = require('./config');
const logger = require('./logger');

// --- Correct WASM Engine Loading ---
// We don't need to manually instantiate. The JS file generated by wasm-pack
// for the 'nodejs' target handles this for us when it's required.
function loadWasm() {
    try {
        logger.info("Loading WASM detection engine...");
        const engine = require('./pkg/mev_engine.js');
        logger.info("‚úÖ Rust/WASM engine loaded successfully.");
        return engine;
    } catch (err) {
        logger.fatal({ err }, "‚ùå Failed to load critical WASM module. This is often an issue with the build process or file paths. Exiting.");
        process.exit(1);
    }
}

// --- Main Application ---
async function main() {
    logger.info("üöÄ Starting MEV-Bot Detector Service in production mode...");

    // 1. Initialize components (WASM loading is now synchronous)
    const mevEngine = loadWasm(); 

    const provider = new ethers.WebSocketProvider(config.wssUrl);
    const kafka = new Kafka({ clientId: 'mev-detector', brokers: [config.kafkaBroker] });
    const producer = kafka.producer({
        retry: { retries: 5, initialRetryTime: 300 }
    });
    const redis = createClient({ url: config.redisUrl });

    // 2. Establish connections with retry logic
    try {
        await producer.connect();
        await redis.connect();
        logger.info("‚úÖ Connected to Kafka and Redis.");
    } catch (err) {
        logger.fatal({ err }, "Could not connect to Kafka or Redis. Exiting.");
        process.exit(1);
    }

    // 3. Set up the transaction processing pipeline
    let txPool = [];
    const processingInterval = setInterval(() => {
        if (txPool.length > 0) {
            const batch = txPool.splice(0, txPool.length); // Atomically get and clear the pool
            processTransactionBatch(batch, mevEngine, producer, redis);
        }
    }, config.batchIntervalMs);

    // 4. Set up provider event listeners
    // Helper: delay for ms
    function sleep(ms) {
        return new Promise(resolve => setTimeout(resolve, ms));
    }

    // Helper: fetch tx with retry
    async function fetchTxWithRetry(txHash, retries = 3, delayMs = 200) {
        for (let i = 0; i < retries; i++) {
            try {
                const tx = await provider.getTransaction(txHash);
                if (tx) return tx;
            } catch (error) {
                // Only retry for internal error
                if (error.code !== -32000 && error.code !== 'UNKNOWN_ERROR') throw error;
            }
            await sleep(delayMs);
        }
        return null;
    }

    provider.on('pending', async (txHash) => {
        try {
            const tx = await fetchTxWithRetry(txHash, 3, 200);
            if (tx && tx.to) {
                txPool.push({
                    hash: tx.hash,
                    from: tx.from,
                    to: tx.to,
                    value: tx.value.toString(),
                    data: tx.data,
                    gasPrice: tx.gasPrice ? tx.gasPrice.toString() : '0',
                });
            }
        } catch (error) {
            // Ignore common mempool errors
            if (error.code !== 'REPLACEMENT_UNDERPRICED' && error.code !== 'NONCE_EXPIRED' && error.code !== 'TIMEOUT') {
                 logger.warn({ txHash, error: error.message }, `Could not fetch tx details`);
            }
        }
    });
    
    provider.on('error', (err) => {
        logger.error({ err }, "Ethereum provider error. The service will attempt to continue but may be degraded.");
    });
    
    redis.on('error', err => logger.error({ err }, 'Redis client error'));

    logger.info({ batchSize: config.batchSize, interval: config.batchIntervalMs }, "üéß Listening for pending transactions...");

    // 5. Graceful shutdown handler
    const gracefulShutdown = async (signal) => {
        logger.warn(`Received ${signal}. Shutting down gracefully...`);
        clearInterval(processingInterval);
        provider.destroy();
        await producer.disconnect();
        await redis.quit();
        logger.info("‚úÖ All connections closed. Exiting.");
        process.exit(0);
    };

    process.on('SIGINT', () => gracefulShutdown('SIGINT'));
    process.on('SIGTERM', () => gracefulShutdown('SIGTERM'));
}


// --- Core Logic Functions (Unchanged) ---
async function processTransactionBatch(txBatch, mevEngine, producer, redis) {
    if (txBatch.length === 0) return;
    logger.debug(`Processing batch of ${txBatch.length} transactions...`);

    try {
        const attackJson = mevEngine.detect_mev(JSON.stringify(txBatch));
        const detectedAttacks = JSON.parse(attackJson);

        if (detectedAttacks.length > 0) {
            logger.info({ count: detectedAttacks.length }, `üî• Detected potential sandwich attack(s)!`);
            for (const attack of detectedAttacks) {
                await handleDetectedAttack(attack, producer, redis);
            }
        }
    } catch (err) {
        logger.error({ err }, "Error processing transaction batch with WASM engine");
    }
}

async function handleDetectedAttack(attack, producer, redis) {
    const attackerAddress = attack.attacker;
    const redisKey = `mev:alert:${attackerAddress}`;

    const isNew = await redis.set(redisKey, 'true', { EX: 300, NX: true });
    if (!isNew) {
        logger.info({ attacker: attackerAddress }, `ü§´ Duplicate alert suppressed for attacker.`);
        return;
    }

    const history = await checkAttackerHistory(attackerAddress);
    if (!history.isSuspicious) {
        logger.info({ attacker: attackerAddress, reason: history.reason }, "Attacker has no suspicious history. Monitoring.");
    }

    logger.warn({ attacker: attackerAddress, victim: attack.victim_tx_hash }, `üö® New valid MEV attacker found. Sending alert!`);

    const alertMessage = {
        victim_tx_hash: attack.victim_tx_hash,
        attacker: attackerAddress,
        frontrun_tx_hash: attack.frontrun_tx_hash,
        backrun_tx_hash: attack.backrun_tx_hash,
        profit_eth: "0.0",
        timestamp: Math.floor(Date.now() / 1000),
        historical_context: history,
    };

    await producer.send({
        topic: config.kafkaTopic,
        messages: [{ value: JSON.stringify(alertMessage) }],
    });
}

async function checkAttackerHistory(address) {
    const query = gql`
      query ($address: Bytes!) {
        swaps(
          first: 20, 
          where: { from: $address }, 
          orderBy: timestamp, 
          orderDirection: desc
        ) {
          id
          timestamp
          pair { id }
        }
      }
    `;
    try {
        const data = await request(config.uniswapSubgraphUrl, query, { address: address.toLowerCase() });
        if (data.swaps.length < 2) {
            return { isSuspicious: false, reason: "Not enough historical swaps." };
        }
        
        for (let i = 0; i < data.swaps.length - 1; i++) {
            const t1 = parseInt(data.swaps[i].timestamp);
            const t2 = parseInt(data.swaps[i+1].timestamp);
            if (Math.abs(t1 - t2) <= 36) {
                return { isSuspicious: true, reason: `Found swaps ${Math.abs(t1-t2)}s apart.`, swapCount: data.swaps.length };
            }
        }
        return { isSuspicious: false, reason: "No rapid sequential swaps found.", swapCount: data.swaps.length };
    } catch (err) {
        logger.error({ err, address }, `‚ùå Error querying The Graph for address history.`);
        return { isSuspicious: false, reason: "Failed to query historical data." };
    }
}

// --- Entry Point ---
main().catch(err => {
    logger.fatal({ err }, "‚ùå Unhandled error in main function. Service is stopping.");
    process.exit(1);
});
